{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14014fd0",
   "metadata": {},
   "source": [
    "## Tutorials\n",
    "\n",
    "1. Get used to DL frameworks: https://pytorch.org/tutorials/beginner/basics/\n",
    "2. Tutorial PyTorch: https://pytorch-geometric.readthedocs.io/en/latest/\n",
    "    - focus on MessagePassing, DataSets and Deploy\n",
    "3. Check Felix examples, including onnx\n",
    "\n",
    "Notebook shortcuts: https://towardsdatascience.com/jypyter-notebook-shortcuts-bf0101a98330"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a78eb",
   "metadata": {},
   "source": [
    "## High performance computation\n",
    "\n",
    "Computations in PyTorch can be performend in 3 ways:\n",
    "1. cpu: standard computation unit (no parallelization)\n",
    "2. cuda: NVDIA's API to parallelize calculations in the NVDIA's GPU\n",
    "3. mps (**M**edal **P**erformance **S**haders): framework to parallelize calculations on MacOS. This maps Machine Learning computational graphs and primitives on highly efficient Metal Performance Shaders Graph framework and tuned kernels provided by Metal Performance Shaders framework respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251047bd",
   "metadata": {},
   "source": [
    "### DeepLearning with PyTorch \n",
    "\n",
    "#### Tensors\n",
    "Source: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html\n",
    "Tensor:\n",
    "- data structure similar to arrays and matrices \n",
    "- in PyTorch, tensors are used to encode inputs, outputs and model parameters\n",
    "- tensors are similar to numpy's arrays, except that tensors can run on GPUs (a tensor shares the underlying memory)\n",
    "- tensors are created on the CPU by default, but can be moved to the gpu if available (CUDA: NVIDIA's SW layer that provides access to the GPU, MPS: framework to parallelize calculations on MacOS)\n",
    "- tensors are optimized for automatic differentiation\n",
    "- tensors feature the same built-in operations as the arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdddba63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "tensor([[0.7452, 0.0525, 0.3887],\n",
      "        [0.0733, 0.2884, 0.1254]])\n",
      "\n",
      "tensor attributes\n",
      "Shape of tensor: torch.Size([2, 3])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n",
      "\n",
      "numpy-like operations\n",
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "\n",
      "join tensors\n",
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
      "\n",
      "matrix multiplication: tensor @ tensor OR tensor.matmul(tensor)\n",
      "original tensor: tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[ 7, 10],\n",
      "        [15, 22]]) tensor([[ 7, 10],\n",
      "        [15, 22]])\n",
      "\n",
      "element-wise product: tensor * tensor OR tensor.mul(tensor)\n",
      "tensor([[ 1,  4],\n",
      "        [ 9, 16]]) tensor([[ 1,  4],\n",
      "        [ 9, 16]])\n",
      "\n",
      "sum tensor data with tensor.sum(): 10\n",
      "\n",
      "in-place oeprations have suffix _: add 5 with tensor.add_(5)\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [8, 9]])\n",
      "\n",
      "numpy array linked to tensor\n",
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# We move our tensor to the GPU if available\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# from python array\n",
    "data = [[1,2], [3,4]]\n",
    "data_tensor = torch.tensor(data)\n",
    "\n",
    "# from numpy array\n",
    "data_np = np.array(data)\n",
    "data_tensor_from_np = torch.from_numpy(data_np)\n",
    "\n",
    "# random tensors\n",
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(shape) # same with .zeros and .ones\n",
    "print(rand_tensor)\n",
    "\n",
    "# attributes: shape, datatype, storage\n",
    "print(\"\\ntensor attributes\")\n",
    "print(f\"Shape of tensor: {rand_tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {rand_tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {rand_tensor.device}\")\n",
    "    \n",
    "# numpy-like operations\n",
    "print(\"\\nnumpy-like operations\")\n",
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)\n",
    "\n",
    "# join tensors\n",
    "print(\"\\njoin tensors\")\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "\n",
    "# aritmetic operations\n",
    "print(\"\\nmatrix multiplication: tensor @ tensor OR tensor.matmul(tensor)\")\n",
    "print(f\"original tensor: {data_tensor}\")\n",
    "print(data_tensor@data_tensor, data_tensor.matmul(data_tensor))\n",
    "print(\"\\nelement-wise product: tensor * tensor OR tensor.mul(tensor)\")\n",
    "print(data_tensor*data_tensor, data_tensor.mul(data_tensor))\n",
    "print(f\"\\nsum tensor data with tensor.sum(): {data_tensor.sum()}\")\n",
    "\n",
    "# in-place operations: result is stored into the operand: memory saving, but no history saved -> no for derivatives\n",
    "print(\"\\nin-place oeprations have suffix _: add 5 with tensor.add_(5)\")\n",
    "print(f\"{data_tensor}\")\n",
    "data_tensor.add_(5)\n",
    "print(data_tensor)\n",
    "\n",
    "# numpy array linked to tensor\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "np.add(n, 1, out=n)\n",
    "print(\"\\nnumpy array linked to tensor\")\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc5fd4",
   "metadata": {},
   "source": [
    "### Datasets and dataloaders\n",
    "Source: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "Dataset code is decoupled from the model training code for better readability and modularity (maintainability).\n",
    "\n",
    "PyTorch provides two data primitives: \n",
    "- torch.utils.data.Dataset: DataLoader wraps an iterable around the Dataset to enable easy access to the samples\n",
    "- torch.utils.data.DataLoader: API to pass batches of data to train the model\n",
    "\n",
    "A custom Dataset class must implement three functions: __init__, __len__, and __getitem__:\n",
    "- __init__(): We initialize the directory containing the images, the annotations file, and both transforms \n",
    "- __len__(): returns the size of the dataset\n",
    "- __getitem__(idx): returns the item (and its label) of the dataset located at the index idx\n",
    "\n",
    "While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.\n",
    "\n",
    "DataLoader is an **iterable** that abstracts this complexity for us in an easy API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9cd7a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## custom dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file) # load labels of data from file\n",
    "        self.img_dir = img_dir # path to data (folder of pictures)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "    \n",
    "# dataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5555c2f4",
   "metadata": {},
   "source": [
    "### Transforms\n",
    "Source: https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html\n",
    "\n",
    "PyTorch's **Transorms** are necessary to **prepare the data to train / use the Machine Learning model**.\n",
    "\n",
    "TorchVision datasets have two parameters:\n",
    "1. transform: function that modifies the features of the data. Many in-built functions can be directly used (see ToTensor())\n",
    "2. target_transform: function that modifies the label of the data\n",
    "\n",
    "In general, built-in functions, lambdas or user-defined functions can be passed to these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f36c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e0f6d",
   "metadata": {},
   "source": [
    "### Build the neural network\n",
    "Source: https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "\n",
    "Neural networks comprise of **layers/modules that perform operations on data**. A neural network is a **module itself that consists of other modules (layers)**. This nested structure allows for building and managing complex architectures easily. \n",
    "\n",
    "The **torch.nn** namespace **provides all the building blocks you need to build your own neural network**. This means that *the neural network is defined by using the subclasses of torch.nn only*.\n",
    "\n",
    "The Neural Network is a **class()** that **inherits from nn.Module**. The class has the following methods:\n",
    "- __init__(): initialization of all the layers of the neural network\n",
    "- __forward__(): defines the operations on the input data\n",
    "\n",
    "To use the model, we pass it the input data. This executes the model’s forward, along with some background operations. Do not call model.forward() directly!\n",
    "\n",
    "EXAMPLE TUTORIAL: Neural Network that classifies the images in the FashionMNIST dataset.\n",
    "\n",
    "**TO DO: understand softmax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e121fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "The device is used when we create the instance of the neural network\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# import module for neural networks\n",
    "from torch import nn\n",
    "\n",
    "# import DataLoader for batching the data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# import dataset library to store the dataset in pytorch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "print(\"The device is used when we create the instance of the neural network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "882a93bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the layers of the Neural Network.\"\"\"\n",
    "        \n",
    "        super().__init__() # needed to inherit functions\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # nn.Sequential is the container of all the NN layers to be executed\n",
    "        # the sequence of layer is defined in nn.Sequential\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features = 28*28, out_features = 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 512, out_features = 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 512, out_features = 10), # output vector of size 10 (or 1x10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Define the operations on the inut data.\"\"\"\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e04b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create the NN object and move it to the defined device\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c15c27f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1088, -0.0136, -0.0297,  0.0100,  0.0193, -0.0565,  0.0295, -0.0791,\n",
      "          0.0037,  0.1063]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "Predicted class: tensor([0], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# create a test input data: tensor of size 1x28x28, which means 1 image of size 28x28\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "\n",
    "# use the non-trained model and get the output 1x10 vector (see layer definition)\n",
    "logits = model(X)\n",
    "print(logits)\n",
    "\n",
    "# apply softmax to select the class of the output vector with highest probability\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9b7b6",
   "metadata": {},
   "source": [
    "#### Understand the layers of the Neural Network\n",
    "\n",
    "1. **nn.Flatten**: transforms the input image (2D array) into a 1D array (from 28x28 to 784)\n",
    "2. **nn.Linear**: apply a linear transformation to the input data using its stored (default) weights and biases. Input data size and output size must be defined.\n",
    "3. **nn.ReLU**: introduction of a non-linear transformation after the linear layer. Non-linear activations are what create the complex mappings between the model’s inputs and outputs. This nonlinearity helps neural networks learn a wide variety of phenomena. Often, ReLU is simply an if condition: {return x if x>=0, otherwise return 0}\n",
    "4. **nn.Sequential**: ordered container of modules (here we define the ordered layers of the Neural Network to be executed) --> here we actually define our Neural Network\n",
    "5. **nn.Softmax**: last linear layer of the neural network. Transforms the values of the input vector into a **vector of probabilities**. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. dim parameter indicates the dimension along which the values must sum to 1.\n",
    "![image](images/softmax.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26b2ea06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test the nn.Flatten layer\n",
      "Original input image size: torch.Size([3, 28, 28])\n",
      "Size of data after the flatten layer: torch.Size([3, 784])\n",
      "\n",
      "Test the nn.Linear layer\n",
      "Size of the input data of the linear layer: torch.Size([3, 784])\n",
      "Size of the output data of the linear layer: torch.Size([3, 20])\n",
      "\n",
      "Test the nn.ReLU layer (nonlinearity: keep the positive values only)\n",
      "Before ReLU: tensor([ 0.2340,  0.0459, -0.2722,  0.1563, -0.5777, -0.0818,  0.5153, -0.5477,\n",
      "        -0.5571,  0.0058,  0.9944, -0.0574,  0.0659, -0.0499, -0.0214, -0.1936,\n",
      "         0.0496, -0.7603,  0.2782, -0.3329], grad_fn=<SelectBackward0>)\n",
      "\n",
      "After ReLU: tensor([0.2340, 0.0459, 0.0000, 0.1563, 0.0000, 0.0000, 0.5153, 0.0000, 0.0000,\n",
      "        0.0058, 0.9944, 0.0000, 0.0659, 0.0000, 0.0000, 0.0000, 0.0496, 0.0000,\n",
      "        0.2782, 0.0000], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Test the nn.Sequential (the neural network) \n",
      "Input of the NN model: torch.Size([1, 28, 28])\n",
      "Output of the NN model (one 10x1 logit for each image): tensor([[-0.1641, -0.0368,  0.1410,  0.0281,  0.0841, -0.0500,  0.1305,  0.0321,\n",
      "         -0.2024, -0.3761]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Test the nn.Softmax layer \n",
      "Input of the Softmax: tensor([[-0.1641, -0.0368,  0.1410,  0.0281,  0.0841, -0.0500,  0.1305,  0.0321,\n",
      "         -0.2024, -0.3761]], grad_fn=<AddmmBackward0>)\n",
      "Output of the Softmax: tensor([[0.0874, 0.0993, 0.1186, 0.1060, 0.1121, 0.0980, 0.1174, 0.1064, 0.0841,\n",
      "         0.0707]], grad_fn=<SoftmaxBackward0>)\n",
      "0.9999999403953552\n",
      "\n",
      "Print the model layers and their parameters \n",
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0236, -0.0138,  0.0297,  ...,  0.0149, -0.0179,  0.0182],\n",
      "        [ 0.0191, -0.0309,  0.0217,  ..., -0.0025,  0.0342, -0.0193]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0088, -0.0254], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0362, -0.0320,  0.0031,  ...,  0.0142,  0.0083, -0.0047],\n",
      "        [ 0.0060, -0.0368,  0.0214,  ...,  0.0298,  0.0343, -0.0425]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0366, 0.0242], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0120, -0.0302,  0.0112,  ...,  0.0175,  0.0200, -0.0394],\n",
      "        [-0.0403, -0.0110,  0.0381,  ...,  0.0077,  0.0389, -0.0341]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0281, 0.0306], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ======================== Test on nn.Flatten ==============================\n",
    "# Let's consider now 3 images of size 28x28\n",
    "input_image = torch.rand(3,28,28)\n",
    "print(f\"\\nTest the nn.Flatten layer\")\n",
    "print(f\"Original input image size: {input_image.size()}\")\n",
    "\n",
    "# flatten layer object\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(f\"Size of data after the flatten layer: {flat_image.size()}\")\n",
    "\n",
    "# ======================== Test on nn.Linear ===============================\n",
    "# Let's consider 3 images of size 28x28\n",
    "print(f\"\\nTest the nn.Linear layer\")\n",
    "linear_layer = nn.Linear(in_features=28*28, out_features=20)\n",
    "linear_layer_output = linear_layer(flat_image)\n",
    "print(f\"Size of the input data of the linear layer: {flat_image.size()}\")\n",
    "print(f\"Size of the output data of the linear layer: {linear_layer_output.size()}\")\n",
    "\n",
    "# ========================= Test nn.ReLU ===================================\n",
    "# Let's consider the output of the linear layer (ReLU is applied after the linear layer)\n",
    "print(f\"\\nTest the nn.ReLU layer (nonlinearity: keep the positive values only)\")\n",
    "print(f\"Before ReLU: {linear_layer_output[0]}\\n\")\n",
    "linear_layer_output = nn.ReLU()(linear_layer_output)\n",
    "print(f\"After ReLU: {linear_layer_output[0]}\")\n",
    "\n",
    "# ========================= Test nn.Sequential ==============================\n",
    "print(f\"\\nTest the nn.Sequential (the neural network) \")\n",
    "\n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    linear_layer,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10) # in_features = 20, out_features = 10\n",
    ")\n",
    "input_image = torch.rand(1,28,28)\n",
    "print(f\"Input of the NN model: {input_image.size()}\")\n",
    "logits = seq_modules(input_image)\n",
    "print(f\"Output of the NN model (one 10x1 logit for each image): {logits}\")\n",
    "\n",
    "# ========================= Test nn.Softmax ==============================\n",
    "# Let's consider the output of the previous Neural Network\n",
    "print(f\"\\nTest the nn.Softmax layer \")\n",
    "print(f\"Input of the Softmax: {logits}\")\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "print(f\"Output of the Softmax: {pred_probab}\")\n",
    "print(sum(pred_probab[0].detach().numpy()))\n",
    "\n",
    "# ======================= Get the model parameters ==========================\n",
    "\n",
    "print(f\"\\nPrint the model layers and their parameters \")\n",
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45943de6",
   "metadata": {},
   "source": [
    "### Gradient descent and backpropagation (autograd) \n",
    "Source: https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "\n",
    "Neural networks are mostly trained with the **gradient descent** algorithm. Here model parameters are calculated in order to **minimize the mean squared error (MSE)** of the loss function. The parameters (model weights) are adjusted according to the *gradient of the loss function* with respect to the given parameter.\n",
    "\n",
    "The gradient descent method involves calculating the derivative of the loss function with respect to the weights of the network. This is normally done using **backpropagation**.\n",
    "\n",
    "#### Gradient descent\n",
    "\n",
    "Source: https://www.ibm.com/topics/gradient-descent\n",
    "\n",
    "The **gradient descent** (also often called **steepest descent**) is a *first-order iterative optimization algorithm* for finding a *local minimum* of a *differentiable function*. \n",
    "\n",
    "The method was created by **Augustin-Louis Cauchy**.\n",
    "\n",
    "To find a local minimum of a differentialble function *f*(**x**), the iterative formula is:\n",
    "\n",
    "**x**_(*k*+1) = **x**_*k* - d * GRAD(*f*(**x**_*k*))\n",
    "\n",
    "where *d* is the learning rate and *k* is the **learning epoch**. In the equation, we see that the best direction to move from **x** is the anti-gradient direction.\n",
    "\n",
    "The gradient descent is an *optimization algorithm* which is commonly-used to train machine learning models and neural networks.  Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates\n",
    "\n",
    "The most important paramenter is the **learning rate** (also referred to as step size or the alpha), which is the size of the steps that are taken to reach the minimum. Usually a small value.\n",
    "![image](images/learning_rate.png)\n",
    "\n",
    "\n",
    "There are 3 types of gradient descent:\n",
    "1. **batch**: all the training data is taken into consideration to take a single step. We take the average of the gradients of all the training examples and then use that mean gradient to update our parameters. So that’s just one step of gradient descent in one epoch. PRO: stability and convergence. CONS: heavy cpu / memory usage\n",
    "2. **stochastic**: just one training data is used to perform a step. PRO: less cpu load & memory usage; noisy gradients, but this can also be helpful in escaping the local minimum and finding the global one. CONS: minimum will not be reached exactly, but the loss value will oscillate around it.\n",
    "3. **mini-batch**: mix of stochastic and batch\n",
    "\n",
    "The gradient descent method has the following limitations:\n",
    "\n",
    "- For convex problems, gradient descent can find the global minimum with ease, but as **nonconvex problems** emerge, gradient descent can *struggle to find the global minimum, where the model achieves the best results*. Infact, when the slope of the cost function is at or close to zero, the model stops learning. \n",
    "- Vanishing gradients: This occurs when the gradient is too small. As we move backwards during backpropagation, the gradient continues to become smaller, causing the earlier layers in the network to learn more slowly than later layers. When this happens, the weight parameters update until they become insignificant—i.e. 0—resulting in an algorithm that is no longer learning.\n",
    "- Exploding gradients: This happens when the gradient is too large, creating an unstable model. In this case, the model weights will grow too large, and they will eventually be represented as NaN. One solution to this issue is to leverage a dimensionality reduction technique, which can help to minimize complexity within the model.\n",
    "\n",
    "#### Backpropagation vs. gradient descent\n",
    "\n",
    "It's very important to clearly distinguish between the two algos:\n",
    "- **gradient descent**: optimization algorithm for minimizing the loss of a predictive model with regard to a training dataset\n",
    "- **back-propagation**: *automatic differentiation algorithm for calculating gradients* for the weights in a neural network graph structure. It uses the *chain rule* to compute the gradient. \n",
    "\n",
    "Source (backpropagation): https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd\n",
    "\n",
    "**gradient**: measures the **sensitivity to change of the function value (output value) with respect to a change in its argument x** (input value)\n",
    "\n",
    "\n",
    "#### Training with PyTorch\n",
    "\n",
    "PyTorch has a built-in differentiation engine called torch.autograd to compute gradients. It supports automatic computation of gradient for any computational graph.\n",
    "\n",
    "To enable autograd, we need to set **requires_grad=True** to all the parameter-tensors we need to optimize, which means for each parameter-tensor **w** for which we must calculate the gradient *d*loss/*d***w**:\n",
    "1. argument **requires_grad=True** when declaring the *variables to be optimized*\n",
    "2. after the variable declaration, we use **.requires_grad_(True)**\n",
    "\n",
    "Then we define the function to be minimized according to the paramenters and we call **function.backward()** to numerically compute the gradient values.\n",
    "\n",
    "We can only perform gradient calculations using backward once on a given graph, for performance reasons. If we need to do several backward calls on the same graph, we need to pass retain_graph=True to the backward call.\n",
    "\n",
    "To optimize weights of parameters in the neural network, we need to **compute the derivatives of our loss function with respect to parameters**, namely *d*loss / *d*w and *d*loss / *d*b.\n",
    "\n",
    "Conceptually, *autograd keeps a record of data (tensors) and all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG)* consisting of Function objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n",
    "\n",
    "#### In summary\n",
    "\n",
    "1. **gradient descent** is the classical algorithm used to calculate the model parameters **p** such that they minimize the loss function *f*_ loss(**x**) = **y** - model(**x**). Here, **x** are the training data (known) and **y** are their labels (known). The optimal parameters **p** are iteratively computed as: **p**_(*k*+1) = **p**_*k* - d * GRAD(*f*_ loss(**p**_*k*)), with d learning rate and k the learning epoch. This is a classical optmizazion algo from numerical calculus (L. Cauchy). Gradient descent can be used as:\n",
    "    - *batch*: for each epoch, all the training data are used to compute the gradients. Then, an average gradient is used.\n",
    "    - *stochastic*: one single training data is used to calculate the gradient in each epoch\n",
    "\n",
    "\n",
    "2. the calculation is the gradients *df*_ loss / *d***p** is done via **backpropagation** (numerical algo to compute the value of the gradient, otherwise impossible using close solutions).\n",
    "\n",
    "3. In PyTorch, there is an built-in engine to compute the gradient, called *torch.autograd*. The parameters of the model that must be optimized shall be defined as tensor with the argument **requires_grad=True**\n",
    "\n",
    "4. We apply the backpropagation with *loss.backward()*, then we get the gradient values as **p**.grad\n",
    "\n",
    "5. When requires_grad=True, PyTorch tracks a lot of gradient values to speed-up the gradient calculation. After the model is trained, we need to disable this tracking with *torch.no_grad()*\n",
    "\n",
    "\n",
    "To compute those derivatives, we call loss.backward(), and then retrieve the values from w.grad and b.grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cda414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We have now object functions to calculate the gradiens:\n",
      "Gradient function for z = <AddBackward0 object at 0x15f878940>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x15f946e30>\n",
      "Gradient dloss/dw (loss: 3x1; w: 5x1)= tensor([[0.2797, 0.2139, 0.2926],\n",
      "        [0.2797, 0.2139, 0.2926],\n",
      "        [0.2797, 0.2139, 0.2926],\n",
      "        [0.2797, 0.2139, 0.2926],\n",
      "        [0.2797, 0.2139, 0.2926]])\n",
      "Gradient dloss/db (loss: 3x1; b: 3x1)= tensor([0.2797, 0.2139, 0.2926])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Consider the simplest one-layer neural network, with input x, parameters w and b, and some loss function. \n",
    "# The model is x*w+b=z, with x: 5x1, w:5x3, b:3x1, z:3x1\n",
    "# Error to be minimized: z - y\n",
    "# Parameters to be optimized: w,b\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "\n",
    "# Machine learning model\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "\n",
    "# Expected output\n",
    "y = torch.zeros(3)\n",
    "\n",
    "# Loss function to be minimized\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "# To optimize the loss function, we need to calculate the *gradient of the loss*\n",
    "print(\"\\nWe have now object functions to calculate the gradiens:\")\n",
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")\n",
    "\n",
    "# Use pytorch built-in backpropagation to compute the gradient of the loss function\n",
    "loss.backward()\n",
    "print(f\"Gradient dloss/dw (loss: 3x1; w: 5x1)= {w.grad}\")\n",
    "print(f\"Gradient dloss/db (loss: 3x1; b: 3x1)= {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75277f9",
   "metadata": {},
   "source": [
    "#### Disabling gradient tracking (often after the training)\n",
    "By default, all tensors with requires_grad=True are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code with torch.no_grad() block.\n",
    "Alternatively, we use detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6698a3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978df46c",
   "metadata": {},
   "source": [
    "### Training the NN model: optimization loop\n",
    "Source: https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "\n",
    "Training the Neural Network is an **interative process** that uses the gradient descent to compute the model parameters that minimize the loss function.\n",
    "\n",
    "We need to set 3 *hyperparameters*:\n",
    "1. **learning rate**: \"scaling factor\" of the gradient in the gradient descent formula\n",
    "2. **batch size**: the number of data samples propagated through the network before the parameters are updated\n",
    "3. **epoch**: number of iterations of the gradient descent. Each epoch consists of two main parts, **which must be manually implemented**:\n",
    "    - **train loop**: interations over the training dataset to converge to optimal parameters\n",
    "    - **validation loop**: iterations over the test dataset to check if model performance is improving\n",
    "\n",
    "In pyTorch, the optimization is done by using built-in classes, thus via initializing objects. These classes are located in **torch.optim**.\n",
    "\n",
    "In order to manually implement the epoch's training loop, we can follow the following steps:\n",
    "1. **reset the gradients** at the beginning of an epoch via **optimizer.zero_grad()**\n",
    "2. **calculate the gradients** with backpropagation via **loss.backpropagate()**\n",
    "3. **do the iteration** with the gradient descent formula via **optimizer.step()**\n",
    "\n",
    "The following code is taken from the previous sections of this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d761956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f9d97a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for the training \n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5 #iterations of the gradient descent\n",
    "\n",
    "# Initialize the Stochastic Gradient Descent (SGD) object\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a389fd9c",
   "metadata": {},
   "source": [
    "### Save, load, use\n",
    "Source: https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b208d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02ca6350",
   "metadata": {},
   "source": [
    "### Torch compile\n",
    "Source: https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html?utm_source=whats_new_tutorials&utm_medium=torch_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53b522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
